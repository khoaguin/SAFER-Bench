# Llama 3.2 1B configuration
model: "meta-llama/Llama-3.2-1B"
device: "auto"  # Automatically use CUDA/MPS/CPU based on availability
max_new_tokens: 10
temperature: 0.7
category: "general"