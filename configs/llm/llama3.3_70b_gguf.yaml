# Llama 3.3 70B Instruct (GGUF Q5_K_M) - General-purpose flagship
# Quantized to 5-bit for excellent quality with reduced memory footprint
#
# MEMORY REQUIREMENTS:
# - Model on disk: ~50GB
# - RAM during inference: ~52GB
#
# GGUF Q5_K_M Benefits:
# - Excellent quality (minimal loss vs FP16)
# - 3x smaller than FP16 (50GB vs 140GB)
# - Metal GPU acceleration on Apple Silicon
# - Fast inference: 20-40 tokens/sec
#
# Model: unsloth/Llama-3.3-70B-Instruct-GGUF
# File: Llama-3.3-70B-Instruct-Q5_K_M.gguf
model: "unsloth/Llama-3.3-70B-Instruct-GGUF"
gguf_file: "Llama-3.3-70B-Instruct-Q5_K_M.gguf"
device: "auto"
max_new_tokens: 50
temperature: 0.7
category: "general"
model_type: "gguf"
