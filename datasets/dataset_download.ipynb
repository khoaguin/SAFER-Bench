{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "from loguru import logger\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "DATASET_DIR = Path.cwd()\n",
    "CORPUS_NAMES = [\"statpearls\", \"textbooks\"]\n",
    "DATASET_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_corpus(corpus_name: str, use_subset: bool = True):\n",
    "    \"\"\"\n",
    "    Download the specified corpus from\n",
    "    https://huggingface.co/datasets/khoaguin/medical-corpus\n",
    "    \"\"\"\n",
    "    if use_subset:\n",
    "        DATASET_PATH = DATASET_DIR / \"subsets\" / corpus_name\n",
    "        allow_patterns = f\"subsets/{corpus_name}/*\"\n",
    "    else:\n",
    "        DATASET_PATH = DATASET_DIR / \"full\" / corpus_name\n",
    "        allow_patterns = f\"{corpus_name}/*\"\n",
    "\n",
    "    if DATASET_PATH.exists():\n",
    "        logger.warning(\n",
    "            f\"Corpus {corpus_name} already exists at {DATASET_PATH}. Skipping download.\"\n",
    "        )\n",
    "        _log_dataset_structure(DATASET_PATH, corpus_name)\n",
    "        return\n",
    "\n",
    "    logger.info(f\"Downloading {corpus_name} to {DATASET_PATH}...\")\n",
    "    snapshot_download(\n",
    "        repo_id=\"khoaguin/medical-corpus\",\n",
    "        repo_type=\"dataset\",\n",
    "        local_dir=DATASET_DIR,\n",
    "        allow_patterns=allow_patterns,\n",
    "    )\n",
    "    _log_dataset_structure(DATASET_PATH, corpus_name)\n",
    "\n",
    "\n",
    "def _log_dataset_structure(dataset_path: Path, corpus_name: str):\n",
    "    \"\"\"Log dataset structure in a clean tree format with sizes\"\"\"\n",
    "\n",
    "    logger.info(f\"ðŸ“Š Dataset structure for {corpus_name}:\")\n",
    "\n",
    "    def get_dir_stats(path):\n",
    "        \"\"\"Get file count and size for a directory\"\"\"\n",
    "        files = list(path.rglob(\"*\"))\n",
    "        file_count = sum(1 for f in files if f.is_file())\n",
    "        total_size_mb = sum(f.stat().st_size for f in files if f.is_file()) / (\n",
    "            1024 * 1024\n",
    "        )\n",
    "        return file_count, total_size_mb\n",
    "\n",
    "    def format_size(size_mb):\n",
    "        \"\"\"Format size in MB or KB\"\"\"\n",
    "        if size_mb < 1:\n",
    "            return f\"{size_mb * 1024:.2f} KB\"\n",
    "        return f\"{size_mb:.2f} MB\"\n",
    "\n",
    "    # Process mock and private directories\n",
    "    for subdir_name in [\"mock\", \"private\"]:\n",
    "        subdir = dataset_path / subdir_name\n",
    "        if subdir.exists():\n",
    "            file_count, size_mb = get_dir_stats(subdir)\n",
    "            logger.info(\n",
    "                f\"  ðŸ“ {subdir_name}/ - {file_count} files ({format_size(size_mb)})\"\n",
    "            )\n",
    "\n",
    "            # Show chunk subdirectory with size\n",
    "            chunk_subdir = subdir / \"chunk\"\n",
    "            if chunk_subdir.exists():\n",
    "                chunk_files = list(chunk_subdir.glob(\"*.jsonl\"))\n",
    "                if chunk_files:\n",
    "                    chunk_size = sum(f.stat().st_size for f in chunk_files) / (\n",
    "                        1024 * 1024\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"    ðŸ“ chunk/ - {len(chunk_files)} jsonl files  ({format_size(chunk_size)})\"\n",
    "                    )\n",
    "\n",
    "            # Show key files with sizes\n",
    "            for filename in [\"all_doc_ids.npy\", \"faiss.index\", \"README.md\"]:\n",
    "                file_path = subdir / filename\n",
    "                if file_path.exists():\n",
    "                    file_size_mb = file_path.stat().st_size / (1024 * 1024)\n",
    "                    logger.info(f\"    ðŸ“„ {filename} ({format_size(file_size_mb)})\")\n",
    "\n",
    "    # Total summary\n",
    "    total_files = sum(1 for _ in dataset_path.rglob(\"*\") if _.is_file())\n",
    "    total_size_mb = sum(\n",
    "        _.stat().st_size for _ in dataset_path.rglob(\"*\") if _.is_file()\n",
    "    ) / (1024 * 1024)\n",
    "    logger.info(f\"âœ… Total: {total_files} files, {format_size(total_size_mb)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for corpus in CORPUS_NAMES:\n",
    "    download_corpus(corpus, use_subset=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "safer-bench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
